{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPl5TkfeCDZI4JcSKY5oS3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenyanglyu/TextMining1/blob/main/Copy_of_TextMining1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YliiM8RZwPP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import  nltk\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Navigate to the folder containing your data\n",
        "folder_path = \"/content/drive/My Drive/textmining/terrorism data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the files and extract them into a list\n",
        "TerrorismList = []\n",
        "for filename in os.listdir(folder_path):\n",
        "  if filename.endswith('.txt'):  # Filter for text files\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r') as f:\n",
        "      data = f.read()\n",
        "      TerrorismList.append(data)"
      ],
      "metadata": {
        "id": "E_ZDMwU5iS2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract bigrams\n",
        "text = nltk.word_tokenize(TerrorismList)\n",
        "def find_bigrams(input_list):\n",
        "  bigram_list = []\n",
        "  for i in range(len(input_list)-1):\n",
        "      bigram_list.append((input_list[i], input_list[i+1]))\n",
        "\n",
        "  return bigram_list\n",
        "#get individual items from the bigram\n",
        "bigramsTerrorismList = find_bigrams(text)"
      ],
      "metadata": {
        "id": "QGa6is-ziZ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count and sort objects\n",
        "from collections import Counter\n",
        "object_counts = Counter(TerrorismList)\n",
        "print(object_counts)"
      ],
      "metadata": {
        "id": "chWye4sbijta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "removeStopWords = []\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words_NLTK = set(stopwords.words('english'))\n",
        "print(stop_words_NLTK)\n",
        "tokens = tokenize(text)\n",
        "removeStopWords = [i for i in tokens if not i in stop_words_NLTK]"
      ],
      "metadata": {
        "id": "gqDYldr_ivy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stem extraction\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "stemTerrorismList = []\n",
        "text_tokens=word_tokenize(text)\n",
        "for word in text_tokens:\n",
        "    print(stemmer.stem(word))\n",
        "    stemTerrorismList.append(stemmer.stem(word))"
      ],
      "metadata": {
        "id": "dJb8saiicGws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma extraction\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmaTerrorismList = []\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "for word in text_tokens:\n",
        "    print(lemmatizer.lemmatize(word))\n",
        "    lemmaTerrorismList.append(stemmer.stem(word))"
      ],
      "metadata": {
        "id": "FsGFSt9-ivRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the trigram in the list\n",
        "from nltk import ngrams\n",
        "n = 3\n",
        "gramsList = ngrams(text.split(), n)\n",
        "trigramsTerrorismList = []\n",
        "for grams in gramsList:\n",
        "  trigramsTerrorismList.append(grams)\n",
        "print(trigramsTerrorismList)"
      ],
      "metadata": {
        "id": "Hok7SbgJrugf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out the most common trigram\n",
        "from collections import Counter\n",
        "trigram_counts = Counter(ngramsTerrorismList)\n",
        "most_common_trigram = trigram_counts.most_common(1)[0][0]\n",
        "print(most_common_trigram)"
      ],
      "metadata": {
        "id": "A6e4tpKzswDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out the two most common bigram\n",
        "from collections import Counter\n",
        "bigram_counts = Counter(bigramsTerrorismList)\n",
        "two_most_common = bigram_counts.most_common(2)\n",
        "\n",
        "# Extract list of the four ngrams\n",
        "from nltk import ngrams\n",
        "m = 4\n",
        "gramsList = ngrams(text.split(), m)\n",
        "fourgramsTerrorismList = []\n",
        "for grams in gramsList:\n",
        "  fourgramsTerrorismList.append(grams)\n",
        "\n",
        "# Find 4-grams containing both bigrams\n",
        "most_common_bigram1 = two_most_common_bigrams[0][0]\n",
        "most_common_bigram2 = two_most_common_bigrams[1][0]\n",
        "\n",
        "matching_4grams = []\n",
        "for fourgram in fourgramsTerrorismList:\n",
        "  # Check if both bigrams are present using all()\n",
        "  if all(bigram in fourgram for bigram in [most_common_bigram1, most_common_bigram2]):\n",
        "    matching_4grams.append(fourgram)\n",
        "\n",
        "# Now matching_4grams contains the desired 4-grams\n",
        "print(\"Matching 4-grams:\", matching_4grams)\n"
      ],
      "metadata": {
        "id": "eyU2GiSIthAP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}